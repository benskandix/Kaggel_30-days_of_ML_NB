{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "# import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/gwbz3fsgp8-2.zip\"\n",
    "# wget.download(url, 'D:\\...\\Datasets')\n",
    "\n",
    "# wget <https://www.kaggle.com/anthonypino/melbourne-housing-market/download/archive.zip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filepath to variable for easier access\n",
    "melbourne_file_path = 'D:\\...\\Datasets\\melb_data.csv'\n",
    "# read the data and store data in DataFrame titled melbourne_data\n",
    "melbourne_data = pd.read_csv(melbourne_file_path) \n",
    "# print a summary and the head of the data in Melbourne data \n",
    "# print(melbourne_data.describe())\n",
    "# print(melbourne_data.head())\n",
    "# print(melbourne_data.columns)\n",
    "# print(melbourne_data.items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with missing values\n",
    "melbourne_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "# Labels\n",
    "y = melbourne_data.Price\n",
    "# Costumized features as inputs\n",
    "melbourne_features1 = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "# All the features as inputs\n",
    "melbourne_features2 = melbourne_data.drop(columns=['Price'])\n",
    "X1 = melbourne_data[melbourne_features1]\n",
    "# X1_index_colom = melbourne_features1\n",
    "X2 = melbourne_features2\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X1, y)\n",
    "train_X2, val_X2, train_y, val_y = train_test_split(X2, y)\n",
    "# len(val_y)/len(y), len(val_X)/len(X1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1st method:\n",
    "# # Specify Model\n",
    "# model = DecisionTreeRegressor()\n",
    "# # Fit Model\n",
    "# model.fit(train_X, train_y)\n",
    "# # Make validation predictions and calculate mean absolute error\n",
    "# preds_val = model.predict(val_X)\n",
    "# mae = mean_absolute_error(val_predictions, val_y)\n",
    "# print(\"Validation MAE: {:,.0f}\".format(val_mae))\n",
    "\n",
    "# 2nd method:\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different leaf sizes\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "# Write loop to find the ideal tree size from candidate_max_leaf_nodes\n",
    "best = []\n",
    "for candidate in candidate_max_leaf_nodes:\n",
    "    meanae = get_mae(candidate, train_X, val_X, train_y, val_y)\n",
    "    best.append(meanae)  \n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "best_tree_size = (candidate_max_leaf_nodes[best.index(min(best))])\n",
    "# best_tree_size, min(best)\n",
    "\n",
    "# fitting the model with the best_leaf_size:\n",
    "model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size)\n",
    "model.fit(train_X, train_y)\n",
    "preds_val = model.predict(val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the home-data-for-ml-course \n",
    "X_full = pd.read_csv(r'D:\\...\\Datasets\\home-data-for-ml-course\\train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv(r'D:\\...\\Datasets\\home-data-for-ml-course\\test.csv', index_col='Id')\n",
    "# Using only numerical predictors\n",
    "X_num = X_full.select_dtypes(exclude=['object'])\n",
    "X_test_num = X_test_full.select_dtypes(exclude=['object'])\n",
    "# Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "\n",
    "# Break off training-validation set from numerical training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_num, y, train_size=0.8)\n",
    "# Break off training-validation set from featured training data\n",
    "X_train_features, X_valid_features, y_train, y_valid = train_test_split(X, y, train_size=0.8)\n",
    "# Break off training-validation set from full training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 37)\n",
      "LotFrontage    210\n",
      "MasVnrArea       8\n",
      "GarageYrBlt     64\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Shape of training data (num_rows, num_columns)\n",
    "print(X_train.shape)\n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "# 1st method :\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1) #,inplace=True)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1) #,inplace=True)\n",
    "\n",
    "# 2nd method\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))  # Fit to data, then transform it.\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))  # Only impute all missing values in X, without fitting.\n",
    "# Imputation removed column names; so put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "# 3rd method\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "# Imputation \n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "# Imputation removed column names; so put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1: reduced data\n",
      "62174.93595890411\n",
      "MAE from Approach 2: imputed data\n",
      "62255.95434931506\n"
     ]
    }
   ],
   "source": [
    "# 1st: Define the model with reduced data\n",
    "print(\"MAE from Approach 1: reduced data\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "# 2nd: Define the model with imputed data\n",
    "print(\"MAE from Approach 2: imputed data\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 MAE: 65290\n",
      "Model 2 MAE: 65355\n",
      "Model 3 MAE: 65130\n",
      "Model 4 MAE: 62579\n",
      "Model 5 MAE: 61623\n"
     ]
    }
   ],
   "source": [
    "# Define different Random Forest models\n",
    "model_1 = RandomForestRegressor(n_estimators=50)\n",
    "model_2 = RandomForestRegressor(n_estimators=100)\n",
    "model_3 = RandomForestRegressor(n_estimators=100, criterion='mae')\n",
    "model_4 = RandomForestRegressor(n_estimators=200, min_samples_split=20)\n",
    "model_5 = RandomForestRegressor(n_estimators=100, max_depth=7)\n",
    "\n",
    "models = [model_1, model_2, model_3, model_4, model_5]\n",
    "\n",
    "# Function for comparing different models\n",
    "def score_model(model, X_t=X_train_features, X_v=X_valid_features, y_t=y_train, y_v=y_valid):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "    mae = score_model(models[i])\n",
    "    print(\"Model %d MAE: %d\" % (i+1, mae))\n",
    "    \n",
    "mae = []\n",
    "for i in range(0, len(models)):\n",
    "    mae.append(score_model(models[i]))\n",
    "for i in range(0, len(models)):\n",
    "    if mae[i] == min(mae):\n",
    "        best_model = models[i]\n",
    "\n",
    "my_model = best_model\n",
    "# fitting the best model\n",
    "my_model.fit(X_train_features,y_train)\n",
    "my_preds_test = my_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data colomns with cathegorical and numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Method', 'Regionname']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Type', 3), ('Method', 5), ('Regionname', 8)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate target from predictors\n",
    "y = melbourne_data.Price\n",
    "X = melbourne_data.drop(['Price'], axis=1)\n",
    "# Divide data into training and validation subsets\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Selecting categorical columns with relatively low cardinality (= 1 to 15) (convenient but arbitrary) \n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "\n",
    "# Get list of categorical variables\n",
    "print(low_cardinality_cols)\n",
    "# s = (X_train.dtypes == 'object')\n",
    "# object_cols = list(s[s].index)\n",
    "# print(\"Categorical variables:\")\n",
    "# print(object_cols)\n",
    "\n",
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), low_cardinality_cols))\n",
    "d = dict(zip(low_cardinality_cols, object_nunique))\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st method: Drop Categorical Variables\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "# 2nd method: Ordinal Encoding\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[low_cardinality_cols] = ordinal_encoder.fit_transform(X_train[low_cardinality_cols])\n",
    "label_X_valid[low_cardinality_cols] = ordinal_encoder.transform(X_valid[low_cardinality_cols])\n",
    "\n",
    "# 3rd method: OneHotEncoder\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables):\n",
      "182482.47458256525\n",
      "MAE from Approach 2 (Ordinal encoder for categorical variables):\n",
      "180631.3939907834\n",
      "MAE from Approach 3 (One Hot Encoder for categorical variables):\n",
      "177878.71885483872\n"
     ]
    }
   ],
   "source": [
    "# 1st: Define the model\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "# 2nd: Define the model with Ordinal encoded data\n",
    "print(\"MAE from Approach 2 (Ordinal encoder for categorical variables):\")\n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "# 3rd: Define the model with OH encoded data\n",
    "print(\"MAE from Approach 3 (One Hot Encoder for categorical variables):\")\n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save predictions in format used for competition scoring\n",
    "# output = pd.DataFrame({'Id': X_test.index, 'SalePrice': preds_test})\n",
    "# output.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 178112.1407822581\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, low_cardinality_cols)\n",
    "    ])\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)\n",
    "\n",
    "# # Preprocessing of test data, fit model\n",
    "# preds_test = my_pipeline.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE scores:\n",
      " [222597.73887903 202468.83454168 182654.21047619 161393.31001883\n",
      " 158724.26229755]\n",
      "Average MAE score (across experiments):\n",
      "185567.67124265764\n"
     ]
    }
   ],
   "source": [
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(\"MAE scores:\\n\", scores)\n",
    "print(\"Average MAE score (across experiments):\")\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "\n",
    "def get_score(n_estimators):\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),\n",
    "                              ('model', RandomForestRegressor(n_estimators=n_estimators,random_state=0))\n",
    "                             ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                              cv=10,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "    m = scores.mean()\n",
    "    return m\n",
    "\n",
    "n_estimators = [50, 100, 150, 200, 250, 300, 350, 400]\n",
    "results = {}\n",
    "for i in range(0,len(n_estimators)):\n",
    "    results[n_estimators[i]] = get_score(n_estimators[i]) \n",
    "\n",
    "plt.plot(list(results.keys()), list(results.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (eXtreme Gradient Boosting) : a level_wise growth appraoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MAE score: 22019.079663420376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([119441.37, 164351.1 , 187794.45, ..., 153774.11, 143085.06,\n",
       "       235366.62], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from the home-data-for-ml-course \n",
    "X_full = pd.read_csv(r'D:\\projets\\AI_py_tutorials\\Datasets\\home-data-for-ml-course\\train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv(r'D:\\projets\\AI_py_tutorials\\Datasets\\home-data-for-ml-course\\test.csv', index_col='Id')\n",
    "# Obtain target and predictors\n",
    "y = X_full.SalePrice\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = X_full[features].copy()\n",
    "X_test = X_test_full[features].copy()\n",
    "# Break off training-validation set from featured training data\n",
    "X_train_features, X_valid_features, y_train, y_valid = train_test_split(X, y, train_size=0.8)\n",
    "\n",
    "\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05) # , n_jobs=4) number of cores!\n",
    "my_model.fit(X_train_features, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_valid_features, y_valid)], \n",
    "             verbose=False)\n",
    "\n",
    "# Get predictions\n",
    "predictions = my_model.predict(X_valid_features)\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_valid, predictions)\n",
    "print(\"Average MAE score: {}\".format(mae))\n",
    "test_predictions = my_model.predict(X_test)\n",
    "test_predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target leakage\n",
    "Target leakage occurs when your predictors include data that will not be available at the time you make predictions.\n",
    "#### Train-Test Contamination\n",
    "A different type of leak occurs when you aren't careful to distinguish training data from validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM (Light Gradient Boost Machine) : a leaf_wise growth appraoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(r'D:\\projets\\AI_py_tutorials\\Datasets\\tabular-playground-series-feb-2021\\train.csv',encoding='utf-8',index_col=0)\n",
    "test = pd.read_csv(r'D:\\projets\\AI_py_tutorials\\Datasets\\tabular-playground-series-feb-2021\\test.csv',encoding='utf-8',index_col=0)\n",
    "\n",
    "y = X['target']\n",
    "X = X.drop(['target'], axis= 1)\n",
    "\n",
    "label = LabelEncoder()\n",
    "categorical_feature = np.where(X.dtypes != 'float64')[0].tolist()\n",
    "categorical_feature_columns = X.select_dtypes(exclude=['float64']).columns\n",
    "\n",
    "for column in categorical_feature_columns:\n",
    "        label.fit(X[column])\n",
    "        X[column] = label.transform(X[column])\n",
    "        test[column] = label.transform(test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_parameters = {\n",
    "    'metric': 'rmse', \n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 50000,\n",
    "    'reg_alpha': 10.924491968127692,\n",
    "    'reg_lambda': 17.396730654687218,\n",
    "    'colsample_bytree': 0.21497646795452627,\n",
    "    'subsample': 0.7582562557431147,\n",
    "    'learning_rate': 0.009985133666265425,\n",
    "    'max_depth': 18,\n",
    "    'num_leaves': 63,    # num_leaves=2^(max_depth)\n",
    "    'min_child_samples': 27,\n",
    "    'max_bin': 523,\n",
    "    'cat_l2': 0.025083670064082797\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optuna\n",
    "# def objective(trial):\n",
    "#     X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25)\n",
    "#     dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "#     param = {\n",
    "#         \"objective\": \"binary\",\n",
    "#         \"metric\": \"binary_logloss\",\n",
    "#         \"verbosity\": -1,\n",
    "#         \"boosting_type\": \"gbdt\",\n",
    "#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "#         \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "#         \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "#         \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "#     }\n",
    "\n",
    "#     gbm = lgb.train(param, dtrain)\n",
    "#     preds = gbm.predict(X_test)\n",
    "#     pred_labels = np.rint(preds)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n",
    "#     return accuracy\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
